{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this we look into the performance of log-domain Sinkhorn for different framework namely: numpy, jax and torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['XLA_FLAGS'] = (\n",
    "#     '--xla_gpu_enable_triton_softmax_fusion=true '\n",
    "#     '--xla_gpu_triton_gemm_any=True '\n",
    "#     '--xla_gpu_enable_async_collectives=true '\n",
    "#     '--xla_gpu_enable_latency_hiding_scheduler=true '\n",
    "#     '--xla_gpu_enable_highest_priority_async_stream=true '\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(1234)\n",
    "\n",
    "import torch\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(1234)\n",
    "                        \n",
    "%matplotlib inline \n",
    "\n",
    "%load_ext autoreload                                                                                                                                                                                            \n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import computational_OT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy regularized formulation\n",
    "\n",
    "The primal entropy regularized formulation of OT is given by:\n",
    "$$\n",
    "OT_{\\varepsilon}(\\alpha,\\beta) = min_{\\pi \\in \\mathcal{U}(\\alpha,\\beta)} \\langle C,\\pi \\rangle +\\varepsilon KL(\\pi\\|\\alpha \\otimes \\beta)\\ ,\n",
    "$$\n",
    "where\n",
    "$\\ \n",
    "KL(\\pi\\|\\alpha \\otimes \\beta) \n",
    "\\ $ is the KL-divergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy regularized dual-formulation\n",
    "The dual formulation of OT is given by:\n",
    "$$\n",
    "OT_{\\varepsilon}(\\alpha,\\beta) = \\max_{f\\in \\mathbb{R}^{n}, g\\in\\mathbb{R}^{m}} \\langle f, \\alpha \\rangle + \\langle g, \\beta \\rangle - \\varepsilon\\left(\\langle\\alpha \\otimes \\beta, e^{\\frac{f}{\\varepsilon}}\\odot K \\odot e^{\\frac{g}{\\varepsilon}}  \\rangle-1\\right)\\ ,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\alpha \\in \\mathcal{M}_{1}(\\mathcal{X}),\\ \\beta \\in \\mathcal{M}_{1}(\\mathcal{Y}),\\ \\varepsilon0,\\ f\\in\\mathbb{R}^{n},\\ g\\in \\mathbb{R}^{m}\\ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn\n",
    "The optimal coupling $\\pi^{*}$ has the following form :\n",
    "$$\n",
    "\\pi^{*} = diag(\\alpha)K diag(\\beta)\n",
    "$$\n",
    "and we know that $\\pi^{*}\\mathbb{1}=\\alpha$ and $(\\pi^{*})^{T}\\mathbb{1}=\\beta$.\n",
    "###\n",
    "Therefore, Sinkhorn updates is given by the following alternative projections\n",
    "$$\n",
    "u^{t+1}  \\leftarrow \\frac{1}{K(v^{t}\\odot \\beta)}\\ , \\\\\n",
    "v^{t+1}  \\leftarrow \\frac{1}{K^{T}(u^{t+1}\\odot \\alpha)}\\ , \n",
    "$$\n",
    "where \n",
    "$K = e^{-\\frac{C}{\\varepsilon}}\\in M_{n\\times m}(\\mathbb{R}),\\ \\alpha \\in \\mathbb{R}^{n},\\ \\beta \\in \\mathbb{R}^{m}\\ ,\\ u\\in \\mathbb{R}^{n},\\ v\\in \\mathbb{R}^{m}\\ and \\ (u^{0},v^{0})=(u,v)\\ .$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log-domain Sinkhorn\n",
    "Now, the exp-log regularized update of the Sinkhorn algorithm is as follows:\n",
    "$$\n",
    "m_{i}(g)\\leftarrow \\min_{j}(C_{ij}-g_{j}^{(t)}),\\ \\forall\\  i = 1,\\dots,n\\ ,\n",
    "$$\n",
    "$$\n",
    "f^{(t+1)}_{i}\\leftarrow -\\varepsilon \\log\\left(\\sum_{j=1}^{m}\\exp\\left(\\frac{-\\left(C_{ij}-g_{j}^{(t)}-m_{i}(g)\\right)}{\\varepsilon}\\right)\\beta_{j}\\right)+m_{i}(g),\\ \\forall\\  i=1,\\dots,n\\ ,\n",
    "$$\n",
    "$$\n",
    "m_{j}(f)\\leftarrow \\min_{i}(C_{ij}-f_{i}^{(t+1)}),\\ \\forall\\   j=1,\\dots,m\\\n",
    " ,\n",
    "$$\n",
    "$$\n",
    "g^{(t+1)}_{j}\\leftarrow -\\varepsilon \\log\\left(\\sum_{i=1}^{n}\\exp\\left(\\frac{-\\left(C_{ij}-f_{i}^{(t+1)}-m_{j}(f)\\right)}{\\varepsilon}\\right)\\alpha_{i}\\right)+m_{j}(f),\\ \\forall\\  j=1,\\dots,m\\ ,\n",
    "$$\n",
    "where \n",
    "$K=e^{-C/\\varepsilon} \\in M_{n \\times m}(\\mathbb{R}),\\ $ $\\varepsilon >0,\\ $ $\\alpha \\in \\mathbb{R}^{n},\\ $ $\\beta \\in \\mathbb{R}^{m},\\ $\n",
    "   $f \\in \\mathbb{R}^{n},\\ $ $g \\in \\mathbb{R}^{m}\\ and \\ (f^{(0)},g^{(0)})=(f,g)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log-domain Sinkhorn using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To compute distance matrix\"\"\"\n",
    "def distmat( x, y ):\n",
    "    return np.sum( x**2, 0 )[:,None] + np.sum( y**2, 0 )[None,:] - 2 * x.transpose().dot( y )\n",
    "\n",
    "\"\"\"To Normalise a vector\"\"\"\n",
    "normalize = lambda a: a/np.sum( a )\n",
    "\n",
    "\"\"\"To Compute P\"\"\"\n",
    "def GetP( u, K, v ):\n",
    "    return u[:,None] * K * v[None,:]\n",
    "\n",
    "def plotp( x, col, plt, scale = 200, edgecolors = \"k\" ):\n",
    "  return plt.scatter( x[0,:], x[1,:], s = scale, edgecolors = edgecolors,  c = col, cmap = 'plasma', linewidths = 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N):\n",
    "    \"\"\"\n",
    "     N is a list of the size of the data on x and y\n",
    "    \"\"\"\n",
    "    x = np.random.rand( 2, N[0] ) - 0.5\n",
    "    theta = 2 * np.pi * np.random.rand( 1, N[1] )\n",
    "    r = 0.8 + .2 * np.random.rand( 1, N[1] )\n",
    "    y = np.vstack( ( r * np.cos( theta ), r * np.sin( theta ) ) )\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [  0.5, 0.1, 0.05, 0.01, 0.005, 0.0005 ]\n",
    "N = [ 500, 600 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = generate_data( N )\n",
    "a = normalize( np.ones( N[0] ) )\n",
    "b = normalize( np.ones( N[1] ) )\n",
    "# Log domain Sinkhorn\n",
    "print( \"Log domain Sinkhorn.... \" )\n",
    "results_logSinkhorn = []\n",
    "times_logSinkhorn   = []\n",
    "logsinkhornP        = []\n",
    "  \n",
    "#Cost matrix\n",
    "C = distmat( x, y )\n",
    "for eps in epsilons:\n",
    "\n",
    "  print( \"For epsilon = \"+str(eps)+\":\" )    \n",
    "  \n",
    "  print( \"Doing for (\",N[0], N[1],\").\" )\n",
    "  print( \" |- Iterating\" )\n",
    "\n",
    "  start = time.time()\n",
    "  logsinkhorn = computational_OT.Log_domainSinkhorn( a, b, C, eps )\n",
    "  output = logsinkhorn.update( niter = 500 )\n",
    "  results_logSinkhorn.append( output )\n",
    "  end = time.time()\n",
    "  times_logSinkhorn.append( 1e3 * ( end - start ) )\n",
    "  logsinkhornP.append(GetP( np.exp( output['potential_f']/eps ), np.exp( -C/eps ), np.exp( output['potential_g']/eps ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "\n",
    "plt.subplot( 2, 1, 1),\n",
    "plt.title( \"$||P1 -a||_1+||P1 -b||_1$\" )\n",
    "for i in range( len( results_logSinkhorn) ):\n",
    "  error = np.asarray( results_logSinkhorn[i]['error'] )\n",
    "  plt.plot( error, label = 'Log-sinkhorn for $\\epsilon = $'+ str(epsilons[i]) , linewidth = 2 )\n",
    "plt.yscale( 'log' )\n",
    "plt.legend()\n",
    "plt.xlabel( \"Iterations\" )\n",
    "plt.ylabel( \"Error in log-scale\" )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log-domain Sinkhorn using JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_data( N )\n",
    "a = normalize( np.ones( N[0] ) )\n",
    "b = normalize( np.ones( N[1] ) )\n",
    "\n",
    "# Log domain Sinkhorn using JAX\n",
    "print(\"Log domain Sinkhorn using JAX.... \")\n",
    "results_logSinkhorn_JAX = []\n",
    "times_logSinkhorn_JAX   = []\n",
    "logsinkhornP_JAX     = []      \n",
    "\n",
    "#Cost matrix\n",
    "C = distmat( x, y )  \n",
    "for eps in epsilons:\n",
    "\n",
    "  print( \"For epsilon = \"+str(eps)+\":\" )    \n",
    "  print( \"Doing for (\",N[0], N[1],\").\" )\n",
    "  print( \" |- Iterating\" )\n",
    "\n",
    "  start = time.time()\n",
    "  output = computational_OT.logdomain_sinkhorn_JAX.update( a, b, C, eps)\n",
    "  results_logSinkhorn_JAX.append( output )\n",
    "  end = time.time() \n",
    "  times_logSinkhorn_JAX.append( 1e3 * ( end - start ) )\n",
    "  logsinkhornP_JAX.append( GetP( jnp.exp( output['potential_f']/eps ), jnp.exp ( - C/eps ), jnp.exp( output['potential_g']/eps ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.subplot( 2, 1, 1),\n",
    "plt.title( \"$||P1 -a||_1+||P1 -b||_1$\" )\n",
    "for i in range( len( results_logSinkhorn_JAX ) ):\n",
    "  error = jnp.asarray( results_logSinkhorn_JAX[i]['error'] )\n",
    "  plt.plot( error, label = 'Log-sinkhorn for $\\epsilon = $'+ str(epsilons[i]) , linewidth = 2 )\n",
    "plt.yscale( 'log' )\n",
    "plt.legend()\n",
    "plt.xlabel( \"Iterations\" )\n",
    "plt.ylabel( \"Error in log-scale\" ) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log-domain Sinkhorn using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torchdistmat( x, y ):\n",
    "   return torch.sum( x**2, 0 )[:,None] + torch.sum( y**2 ,0  )[None,:] - 2 * torch.matmul( x.t(), y )\n",
    "torchnormalize = lambda a: a/torch.sum( a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_data( N )\n",
    "x, y = torch.from_numpy( x ), torch.from_numpy( y )\n",
    "a = torchnormalize( torch.ones( N[0] ) )\n",
    "b = torchnormalize( torch.ones( N[1] ) )\n",
    "# Log domain Sinkhorn using torch\n",
    "print(\"Log domain Sinkhorn using torch.... \")\n",
    "results_logSinkhorn_torch = []\n",
    "times_logSinkhorn_torch   = []\n",
    "logsinkhornP_torch        = []\n",
    "\n",
    "#Cost matrix\n",
    "C = torchdistmat( x, y )\n",
    "for eps in epsilons:\n",
    "  print( \"For epsilon = \"+str(eps)+\":\" )    \n",
    "  print( \"Doing for (\",N[0], N[1],\").\" )\n",
    "  print( \" |- Iterating\" )\n",
    "  start = time.time()\n",
    "  optimizer = computational_OT.torchLog_domainSinkhorn( a, b, C, eps )\n",
    "  outputtorchLogSinkhorn = optimizer.update( niter = 500 )\n",
    "  results_logSinkhorn_torch.append( outputtorchLogSinkhorn )\n",
    "  end = time.time()\n",
    "  times_logSinkhorn_torch.append( 1e3 * ( end - start ) )\n",
    "  logsinkhornP_torch.append(GetP( torch.exp( outputtorchLogSinkhorn['potential_f']/eps ), torch.exp( -C/eps ), torch.exp( outputtorchLogSinkhorn['potential_g']/eps ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.title( \"$||P1-a||_1+||P1-b||_1$\" ),\n",
    "for i in range(len(results_logSinkhorn_torch)):\n",
    "    error = results_logSinkhorn_torch[i]['error'] \n",
    "    plt.plot( error, label='Log-domain Sinkhorn for $\\epsilon = $'+str(epsilons[i]), linewidth = 2  )\n",
    "plt.yscale( 'log' )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 10, 4 ) )\n",
    "plt.plot( list(range(len(epsilons))), times_logSinkhorn[::-1], label = 'Log-domain Sinkhorn', marker = 'o', linewidth = 2 )\n",
    "plt.plot( list(range(len(epsilons))), times_logSinkhorn_JAX[::-1], label = 'Log-domain Sinkhorn using JAX', marker = 'o', linewidth = 2 )\n",
    "plt.plot( list(range(len(epsilons))), times_logSinkhorn_torch[::-1], label = 'Log-domain Sinkhorn using torch', marker = 'o', linewidth = 2 )\n",
    "plt.legend()\n",
    "plt.xticks( list(range(len(epsilons))), epsilons[::-1] )\n",
    "plt.xlabel( \"$\\epsilon$\" )\n",
    "plt.yscale( 'log' )\n",
    "plt.ylabel( \"Time in ms\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_computational-OT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
