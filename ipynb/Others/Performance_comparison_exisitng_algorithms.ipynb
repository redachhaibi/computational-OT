{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook we look at the performance of various optimization algorithms namely: linear optimization, Sinkhorn, gradient ascent, line search and L-BFGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sc\n",
    "import pylab as pyl\n",
    "import time\n",
    "import cvxpy as cp\n",
    "from numpy import linalg as Lin\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import style\n",
    "from sklearn import datasets\n",
    "import computational_OT\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path_to_new_folder = \"../Images\"\n",
    "os.makedirs(relative_path_to_new_folder, exist_ok = True)\n",
    "if not os.path.isdir('../Images/Performance_comparison_exisitng_algorithms_images'):\n",
    "    os.makedirs('../Images/Performance_comparison_exisitng_algorithms_images')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-tlrzQ3chK2S"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHDEEFt3Kvkk"
   },
   "outputs": [],
   "source": [
    "\"\"\"To compute distance matrix\"\"\"\n",
    "def distmat( x, y ):\n",
    "    return np.sum( x**2, 0 )[:,None] + np.sum( y**2, 0 )[None,:] - 2 * x.transpose().dot( y )\n",
    "\n",
    "\"\"\"To Normalise a vector\"\"\"\n",
    "normalize = lambda a: a/np.sum( a )\n",
    "\n",
    "\"\"\"To Compute P\"\"\"\n",
    "def GetP( u, K, v ):\n",
    "    return u[:,None] * K * v[None,:]\n",
    "\n",
    "def plotp( plt, x, col, scale = 200, edgecolors = \"k\" ):\n",
    "  return plt.scatter( x[0,:], x[1,:], s = scale, edgecolors = edgecolors, c = col, cmap = 'plasma', linewidths = 2 )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JRHkp03xbECd"
   },
   "source": [
    "## Generate Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6uU8x1qhuo2"
   },
   "outputs": [],
   "source": [
    "\"\"\" Here we generate points from an annulus and a square.\"\"\"\n",
    "def randomsampledata( N ):\n",
    "  x = []\n",
    "  y = []\n",
    "  N = np.sort( N )\n",
    "  for i in range(len(N)):\n",
    "    x.append( np.random.rand( 2, N[i] ) - 0.5 )\n",
    "    theta= 2 * np.pi * np.random.rand( 1, N[i] )\n",
    "    r = 0.8 + 0.2 * np.random.rand( 1, N[i] )\n",
    "    y.append( np.vstack( ( np.cos(theta) * r, np.sin(theta) * r ) ) )\n",
    "  return x, y, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUO4f-EYLx2R"
   },
   "outputs": [],
   "source": [
    "N = [ 200, 400, 600, 800, 1000 ]\n",
    "x, y, N = randomsampledata( N )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primal formulation\n",
    "The primal formulation of the optimal transport problem is given by:\n",
    "$$\n",
    "OT_{\\epsilon}(\\alpha,\\beta) = min_{\\pi \\in \\mathcal{U(\\alpha,\\beta)}} \\langle C, \\pi \\rangle\\ ,\n",
    "$$\n",
    "where \n",
    "$\\ \\mathcal{U(\\alpha,\\beta)}=\\{\\pi: \\pi\\mathbb{1} = \\alpha,\\ \\pi^{T}\\mathbb{1}=\\beta\\}\\ .$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KijJh0En1oxK"
   },
   "source": [
    "## Linear Optimization\n",
    "This primal formulation can be looked as a linear optimization problem and therefore, we look at the performance of the linear optimization algorithm to find the optimal coupling $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SFad1Bvky0yZ",
    "outputId": "0c879d5d-8d0c-4787-8dbb-8a55f6ecce84"
   },
   "outputs": [],
   "source": [
    "# Linear optimization\n",
    "print(\"Linear optimization...\")\n",
    "times_linearOpt = []\n",
    "LinearP = []\n",
    "Error = []\n",
    "for i in range(len(N)):\n",
    "  xi, yi = x[i], y[i]\n",
    "  #Cost matrix\n",
    "  C = distmat( xi, yi )\n",
    "  # a and b\n",
    "  a = normalize( np.ones( N[i] ) ) \n",
    "  b = normalize( np.ones( N[i] ) )\n",
    "  # Plotting point clouds  \n",
    "  plt.figure( figsize = ( 5, 5 ) )\n",
    "  plotp( plt, xi, col = 'b')\n",
    "  plotp( plt, yi, col = 'r' )\n",
    "  plt.axis( \"off\" )\n",
    "  plt.xlim( np.min( yi[0,:] ) - .1, np.max( yi[0,:] ) + .1 )\n",
    "  plt.ylim( np.min( yi[1,:] ) - .1, np.max( yi[1,:] ) + .1 )\n",
    "  plt.show()\n",
    "  # Optimization\n",
    "  print( \"Doing for \",N[i] )\n",
    "  print( \" |- Iterating\" )\n",
    "  start = time.time()\n",
    "  Optimizer = computational_OT.linear_optimization( N[i],\n",
    "                                                    N[i],\n",
    "                                                    a,\n",
    "                                                    b,\n",
    "                                                    C )\n",
    "  print( \" |- Computing P\" )\n",
    "  print( \"\" )\n",
    "  out = Optimizer.solve()\n",
    "  end = time.time()\n",
    "  LinearP.append( out['Optimal coupling'].value )\n",
    "  Error.append( out['Error'] )\n",
    "  times_linearOpt.append( end - start )\n",
    "#end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy regularized formulation\n",
    "\n",
    "The primal entropy regularized formulation of OT is given by:\n",
    "$$\n",
    "OT_{\\epsilon}(\\alpha,\\beta) = min_{\\pi \\in \\mathcal{U}(\\alpha,\\beta)} \\langle C,\\pi \\rangle +\\epsilon KL(\\pi\\|\\alpha \\otimes \\beta)\\ ,\n",
    "$$\n",
    "where\n",
    "$\\ \n",
    "KL(\\pi\\|\\alpha \\otimes \\beta) \n",
    "\\ $ is the KL-divergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy regularized dual-formulation\n",
    "The dual formulation of OT is given by:\n",
    "$$\n",
    "OT_{\\epsilon}(\\alpha,\\beta) = \\max_{f\\in \\mathbb{R}^{n}, g\\in\\mathbb{R}^{m}} \\langle f, \\alpha \\rangle + \\langle g, \\beta \\rangle - \\epsilon\\left(\\langle\\alpha \\otimes \\beta, e^{\\frac{f}{\\epsilon}}\\odot K \\odot e^{\\frac{g}{\\epsilon}}  \\rangle-1\\right)\\ ,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\alpha \\in \\mathcal{M}_{1}(\\mathcal{X}),\\ \\beta \\in \\mathcal{M}_{1}(\\mathcal{Y}),\\ \\epsilon>0,\\ f\\in\\mathbb{R}^{n},\\ g\\in \\mathbb{R}^{m}\\ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy regularized formulation\n",
    "\n",
    "The primal entropy regularized formulation of OT is given by:\n",
    "$$\n",
    "OT_{\\epsilon}(\\alpha,\\beta) = min_{\\pi \\in \\mathcal{U}(\\alpha,\\beta)} \\langle C,\\pi \\rangle +\\epsilon KL(\\pi\\|\\alpha \\otimes \\beta)\\ ,\n",
    "$$\n",
    "where\n",
    "$\\ \n",
    "KL(\\pi\\|\\alpha \\otimes \\beta) \n",
    "\\ $ is the KL-divergence and $\\ \\mathcal{U}(\\alpha,\\beta)=\\{\\pi: \\pi\\mathcal{1}=\\alpha, \\pi^{T}\\mathcal{1}=\\beta\\}$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XI1siOVE1t0l"
   },
   "source": [
    "### I. Sinkhorn\n",
    "The optimal coupling $\\pi^{*}$ has the following form :\n",
    "$$\n",
    "\\pi^{*} = \\alpha \\odot diag(u)K diag(v)\\odot \\beta\n",
    "$$\n",
    "and we know that $\\pi^{*}\\mathbb{1}=\\alpha$ and $(\\pi^{*})^{T}\\mathbb{1}=\\beta$.\n",
    "###\n",
    "Therefore, Sinkhorn updates is given by the following alternative projections\n",
    "$$\n",
    "u^{t+1}  \\leftarrow \\frac{1}{K(v^{t}\\odot \\beta)}\\ ,\\ \n",
    "v^{t+1}  \\leftarrow \\frac{1}{K^{T}(u^{t+1}\\odot \\alpha)}\\ , \n",
    "$$\n",
    "where \n",
    "$K = e^{-\\frac{C}{\\epsilon}}\\in M_{n\\times m}(\\mathbb{R}),\\ \\alpha \\in \\mathbb{R}^{n},\\ \\beta \\in \\mathbb{R}^{m}\\ ,\\ u\\in \\mathbb{R}^{n},\\ v\\in \\mathbb{R}^{m}\\ and \\ (u^{0},v^{0})=(u,v)\\ .$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpF7nb14Kz1y",
    "outputId": "235f0b6f-0bd1-40e1-c1d4-8c283e738af1"
   },
   "outputs": [],
   "source": [
    "# Sinkhorn\n",
    "print(\"Sinkhorn... \")\n",
    "SinkhornP = []\n",
    "results_Sinkhorn = []\n",
    "times_Sinkhorn = []\n",
    "#Epsilon\n",
    "epsilon = .06\n",
    "for i in range(len(N)):\n",
    "  xi, yi = x[i], y[i]\n",
    "  #Cost matrix\n",
    "  C = distmat( xi, yi )\n",
    "  # a and b\n",
    "  a = normalize( np.ones( N[i] ) ) \n",
    "  b = normalize( np.ones( N[i] ) )\n",
    "  #Kernel\n",
    "  K = np.exp( - C/epsilon )\n",
    "  print( \"Doing for \",N[i] )\n",
    "  print( \" |- Iterating\" )\n",
    "  #Inflating\n",
    "  u = a\n",
    "  v = b\n",
    "  start = time.time()\n",
    "  Optimizer = computational_OT.sinkhorn(  K,\n",
    "                                          a,\n",
    "                                          b,\n",
    "                                          u,\n",
    "                                          v,\n",
    "                                          epsilon )\n",
    "  out = Optimizer._update()\n",
    "  results_Sinkhorn.append( out )\n",
    "  end = time.time()\n",
    "  times_Sinkhorn.append( end - start )\n",
    "  print( \" |- Computing P\" )\n",
    "  print( \"\" )\n",
    "  u_opt = np.exp( out['potential_f']/epsilon )\n",
    "  K = np.exp( - C/epsilon )\n",
    "  v_opt =  np.exp( out['potential_g']/epsilon )\n",
    "  P_opt = GetP( u_opt, K, v_opt )\n",
    "  SinkhornP.append( P_opt )\n",
    "# end for  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HIvJCYZEiBjH"
   },
   "source": [
    "##### Error plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "8glqdQVC3C7W",
    "outputId": "51413c80-637c-4fc2-b587-b9e6c86efe41"
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.subplot( 2, 1, 2 )\n",
    "plt.title( \"$||P 1 -a||_1+||P^T 1 -b||_1$\" )\n",
    "for result in results_Sinkhorn:\n",
    "    error = np.asarray( result['error_a'] ) + np.asarray( result['error_b'] ) \n",
    "    plt.plot( error, linewidth = 2 )\n",
    "# end for\n",
    "plt.yscale( 'log' )\n",
    "plt.legend( [ \"N = \"+str(i) for i in N ], loc = \"upper right\" )\n",
    "plt.savefig( \"../Images/Performance_comparison_exisitng_algorithms_images/ConvergenceSinkhorn.pdf\", format = 'pdf' )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SbwY4W2eIysN"
   },
   "source": [
    "##### Objective function plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "T7aD_v82Iod_",
    "outputId": "8cc5f837-29b5-4162-c560-bb6e0724290e"
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.subplot( 2, 1, 1 ),\n",
    "plt.title( \"Objective Function\" )\n",
    "for result in results_Sinkhorn:\n",
    "  plt.plot( np.asarray(result['objective_values']), linewidth = 2 )\n",
    "# end for\n",
    "plt.legend( [ \"N = \"+str(i) for i in N ], loc = \"upper right\" )\n",
    "plt.savefig( \"../Images/Performance_comparison_exisitng_algorithms_images/ObjectiveSinkhorn.pdf\", format = 'pdf' )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dPgMmsHIZA3G"
   },
   "source": [
    "### II. Gradient Ascent\n",
    "Here we perform the gradient ascent algorithm on the dual entropy regularized setup as mentioned before. The gradients of the objective function $Q$ in this setup are as follows:\n",
    "$$\n",
    "\\nabla_{f} Q(f,g) = \\alpha -\\alpha\\odot e^{\\frac{f}{\\epsilon}}\\odot\\langle K, e^{\\frac{g}{\\epsilon}}\\odot \\beta \\rangle\\ , \n",
    "$$\n",
    "$$\n",
    "\\nabla_{g} Q(f,g) = \\beta - \\beta\\odot e^{\\frac{g}{\\epsilon}}\\odot \\langle K, e^{\\frac{f}{\\epsilon}}\\odot \\alpha \\rangle\\ ,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f \\in \\mathbb{R}^{n},\\ g \\in \\mathbb{R}^{m}\\ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U01CNug6ZAil",
    "outputId": "f04af0e8-1e7d-4ee8-cf0d-a0e06e25aa7b"
   },
   "outputs": [],
   "source": [
    "# Gradient ascent\n",
    "print(\"Gradient ascent...\")\n",
    "learning_rate = 0.01\n",
    "#Epsilon\n",
    "epsilon = .06\n",
    "GradientAscentP = []\n",
    "results_Gradient_ascent = []\n",
    "times_Gradient_ascent = []\n",
    "results_Gradient_ascent = []\n",
    "times_Gradient_ascent = []\n",
    "for i in range(len(N)):\n",
    "  xi, yi = x[i], y[i]\n",
    "  #Cost matrix\n",
    "  C = distmat( xi, yi )\n",
    "  # a and b\n",
    "  a = normalize( np.ones( N[i] ) ) \n",
    "  b = normalize( np.ones( N[i] ) )\n",
    "  #Kernel\n",
    "  K = np.exp( - C/epsilon )\n",
    "  f, g = a, b\n",
    "  print( \"Doing for \",N[i] )\n",
    "  print( \" |- Iterating\" )\n",
    "  start = time.time()\n",
    "  Optimizer = computational_OT.gradient_ascent( K,\n",
    "                                                a,\n",
    "                                                b,\n",
    "                                                f,\n",
    "                                                g,\n",
    "                                                epsilon,\n",
    "                                                learning_rate )\n",
    "  out = Optimizer._update()\n",
    "  end = time.time()\n",
    "  results_Gradient_ascent.append( out )\n",
    "  times_Gradient_ascent.append( end - start )\n",
    "  print( \" |- Computing P\" )\n",
    "  print( \"\" )\n",
    "  u_opt = np.exp( out['potential_f']/epsilon )\n",
    "  K = np.exp( - C/epsilon )\n",
    "  v_opt =  np.exp( out['potential_g']/epsilon )\n",
    "  P_opt = GetP( u_opt, K, v_opt )\n",
    "  GradientAscentP.append( P_opt )\n",
    "# end for"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "utM3LQZtiJ1O"
   },
   "source": [
    "##### Error plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "pgeNPq_a3CnB",
    "outputId": "30f58ecf-2e93-419a-a972-7e097576cb5d"
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.subplot( 2, 1, 2 )\n",
    "plt.title( \"$||P 1 -a||_1+||P^T 1 -b||_1$\" )\n",
    "for result in results_Gradient_ascent:\n",
    "    error = np.asarray( result['error_a'] ) + np.asarray( result['error_b'] ) \n",
    "    plt.plot( error, linewidth = 2 )\n",
    "# end for\n",
    "plt.yscale( 'log' )\n",
    "plt.legend( [ \"N = \"+str(i) for i in N ], loc = \"upper right\" )\n",
    "plt.savefig( \"../Images/Performance_comparison_exisitng_algorithms_images/ConvergenceGradient_ascent.pdf\", format = 'pdf' )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dn9Ed-_2JOGZ"
   },
   "source": [
    "##### Objective function plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "nePA5WXKJJ49",
    "outputId": "86f8fae9-9776-4571-e622-4b0b452e28f4"
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.subplot( 2, 1, 1 ),\n",
    "plt.title( \"Objective Function\" )\n",
    "for result in results_Gradient_ascent:\n",
    "  plt.plot( np.asarray(result['objective_values']).flatten(), linewidth = 2 )\n",
    "# end for\n",
    "plt.legend( [ \"N = \"+str(i) for i in N ], loc = \"upper right\" )\n",
    "plt.savefig( \"../Images/Performance_comparison_exisitng_algorithms_images/ObjectiveGradient_ascent.pdf\", format = 'pdf' )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0WIBcjX9tSs6"
   },
   "source": [
    "### III. Line Search\n",
    "Here we use the Armijo condition to obtain the step size to update the current vector toward the update direction which is obtained by gradient ascent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQCyZn8_eZmU",
    "outputId": "1ec9d4f8-4e3a-4de4-9df2-2c2032a65975"
   },
   "outputs": [],
   "source": [
    "# Line Search\n",
    "print( \"Line Search...\" )\n",
    "# Damping factor for ascent step-size\n",
    "rho = 0.95\n",
    "# Damping factor to increase ascent step-size\n",
    "rho_inc = 1.5\n",
    "# Damping factor for the slope in Wolfe condition\n",
    "c1 = 0.05\n",
    "#Epsilon\n",
    "epsilon = .06\n",
    "initial_alpha = 1\n",
    "LineSearchP = []\n",
    "results_LineSearch = []\n",
    "times_LineSearch = []\n",
    "for i in range(len(N)):\n",
    "      xi, yi = x[i], y[i]\n",
    "      #Cost matrix\n",
    "      C = distmat( xi, yi )\n",
    "      # a and b\n",
    "      a = normalize( np.ones( N[i] ) ) \n",
    "      b = normalize( np.ones( N[i] ) )\n",
    "      #Kernel\n",
    "      K = np.exp( - C/epsilon )\n",
    "      f, g = a, b\n",
    "      print( \"Doing for \", N[i] )\n",
    "      print( \" |- Iterating\" )\n",
    "      start = time.time()\n",
    "      Optimizer = computational_OT.linesearch(  K,\n",
    "                                                a,\n",
    "                                                b,\n",
    "                                                f,\n",
    "                                                g,\n",
    "                                                epsilon,\n",
    "                                                rho,\n",
    "                                                rho_inc,\n",
    "                                                c1,\n",
    "                                                initial_alpha )\n",
    "      out = Optimizer._update()\n",
    "      results_LineSearch.append( out )\n",
    "      end = time.time()\n",
    "      times_LineSearch.append( end - start )\n",
    "      print( \" |- Computing P\" )\n",
    "      print( \"\" )\n",
    "      u_opt = np.exp( out['potential_f']/epsilon )\n",
    "      K = np.exp( - C/epsilon )\n",
    "      v_opt =  np.exp( out['potential_g']/epsilon )\n",
    "      P_opt = GetP( u_opt, K, v_opt )\n",
    "      LineSearchP.append( P_opt )\n",
    "# end for"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Error plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "SGAUKWmFZTLj",
    "outputId": "3750281b-f3d7-4f3f-cce9-acdc2b8eecd0"
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.subplot( 2, 1, 2 )\n",
    "plt.title( \"$||P 1 -a||_1+||P^T 1 -b||_1$\" )\n",
    "for result in results_LineSearch:\n",
    "    error = np.asarray( result['error_a'] ) + np.asarray( result['error_b'] ) \n",
    "    plt.plot( error, linewidth = 2 )\n",
    "# end for\n",
    "plt.yscale( 'log' )\n",
    "plt.legend( [ \"N = \"+str(i) for i in N ], loc = \"upper right\" )\n",
    "plt.savefig( \"../Images/Performance_comparison_exisitng_algorithms_images/ConvergenceLineSearch.pdf\", format = 'pdf'  )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Objective function plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "RWs5fDjGeZyb",
    "outputId": "41f894e0-ba8b-41ed-e06e-05278955538c"
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.subplot( 2, 1, 1 ),\n",
    "plt.title( \"Objective Function\" )\n",
    "for result in results_LineSearch:\n",
    "  plt.plot( np.asarray( result['objective_values'] ), linewidth = 2 )\n",
    "# end for\n",
    "plt.legend( [ \"N = \"+str(i) for i in N ], loc = \"upper right\" )\n",
    "plt.savefig( \"../Images/Performance_comparison_exisitng_algorithms_images/ConvergenceLineSearch.pdf\", format = 'pdf'  )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ascent step-size plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "im-fEYZ-pw44",
    "outputId": "28803063-395b-4d99-c481-9a64130faa20"
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.subplot( 2, 1, 1 ),\n",
    "plt.title( \"Alpha\" )\n",
    "for result in results_LineSearch:\n",
    "  plt.plot( np.asarray( result['linesearch_steps'] ), linewidth = 2 )\n",
    "  # end for\n",
    "plt.legend( [ str(i) for i in N ], loc = \"upper right\" )\n",
    "plt.legend( [ \"N = \"+str(i) for i in N ], loc = \"upper right\" )\n",
    "plt.savefig( \"../Images/Performance_comparison_exisitng_algorithms_images/Alphaplot_LineSearch.pdf\", format = 'pdf'  )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For varying epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Search\n",
    "print( \"Line Search...\" )\n",
    "print( \"Doing for (\",N[0], N[1],\").\" )\n",
    "# Damping factor for ascent step-size\n",
    "rho = 0.95\n",
    "# Damping factor to increase ascent step-size\n",
    "rho_inc = 1.5\n",
    "# Damping factor for the slope in Wolfe condition\n",
    "c1 = 0.05\n",
    "initial_alpha = 1\n",
    "LineSearchP = []\n",
    "results_LineSearch = []\n",
    "#Cost matrix\n",
    "C = distmat( x[1], y[1] )\n",
    "# a and b\n",
    "a = normalize( np.ones( N[1] ) )\n",
    "b = normalize( np.ones( N[1] ) )\n",
    "epsilons = [  0.1,  0.08, 0.05, 0.01 ]\n",
    "for eps in epsilons:\n",
    "    print( \"For epsilon = \"+str(eps)+\":\" )    \n",
    "    #Kernel\n",
    "    K = np.exp( - C/eps )\n",
    "    f, g = a, b\n",
    "    print( \" |- Iterating\")\n",
    "    start = time.time()\n",
    "    Optimizer = computational_OT.LineSearch(    K,\n",
    "                                                a, \n",
    "                                                b,\n",
    "                                                f,\n",
    "                                                g,\n",
    "                                                eps,\n",
    "                                                rho,\n",
    "                                                rho_inc,\n",
    "                                                c1,\n",
    "                                                initial_alpha )\n",
    "    out = Optimizer._update( max_iterations = 1000 ) \n",
    "    results_LineSearch.append( out )\n",
    "    end = time.time()\n",
    "    print( \" |- Computing P\" )\n",
    "    print( \"\" )\n",
    "    u_opt = np.exp( out['potential_f']/eps )\n",
    "    K = np.exp( - C/eps )\n",
    "    v_opt =  np.exp( out['potential_g']/eps )\n",
    "    P_opt = GetP( u_opt, K, v_opt )\n",
    "    LineSearchP.append( P_opt )\n",
    "# end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Error plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure( figsize = ( 15, 6 ) )\n",
    "n = len( results_LineSearch )\n",
    "plt.title( \"$||P1 -a||_1 + ||P^T 1 -b||_1$\" )\n",
    "for i in range(n):\n",
    "    error_hybrid = np.asarray( results_LineSearch[i]['error_a'] ) + np.asarray( results_LineSearch[i]['error_b'] )\n",
    "    plt.plot( error_hybrid, label = 'LineSearch for $\\epsilon = $'+ str(epsilons[i]), linewidth = 2)\n",
    "# end for\n",
    "plt.xlabel( \"Number of iterations\" )\n",
    "plt.ylabel( \"Error in log-scale\" )\n",
    "plt.legend( loc = 'upper right' )\n",
    "plt.yscale( 'log' )\n",
    "plt.savefig( \"../Images/Performance_comparison_exisitng_algorithms_images/Linesearchvaryingepsilon.pdf\", format = 'pdf'  )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Objective function plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.subplot( 2, 1, 1 ),\n",
    "plt.title( \"Alpha\" )\n",
    "for i in range(len(results_LineSearch)):\n",
    "  plt.plot( np.asarray( results_LineSearch[i]['linesearch_steps']), label = 'LineSearchNewton for $\\epsilon = $'+ str(epsilons[i]), linewidth = 2 )\n",
    "# end for\n",
    "plt.xlabel( \"Number of iterations\" )\n",
    "plt.legend( loc = 'upper right' )\n",
    "plt.savefig( \"../Images/Performance_comparison_exisitng_algorithms_images/AlphaLineSearchNewton.pdf\", format = 'pdf'  )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B4y9X_pPK9kJ"
   },
   "source": [
    "### IV. L_BFGS_B\n",
    "Here we perform the L-BFGS algorithm on the dual entropy regularized problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4L3OhgpiKm1M",
    "outputId": "3a704484-dea3-4e33-aaf7-a1ab42035db6"
   },
   "outputs": [],
   "source": [
    "# BFGS\n",
    "print(\"BFGS...\")\n",
    "BFGSP = []\n",
    "results_BFGS = []\n",
    "#Epsilon\n",
    "epsilon = .06\n",
    "times_BFGS = []\n",
    "for i in range(len(N)):\n",
    "      xi, yi = x[i], y[i]\n",
    "      #Cost matrix\n",
    "      C = distmat( xi, yi )\n",
    "      # a and b\n",
    "      a = normalize( np.ones( N[i] ) )\n",
    "      a = a.reshape( a.shape[0], - 1 )\n",
    "      b = normalize( np.ones( N[i] ) )\n",
    "      b = b.reshape( b.shape[0], - 1 )\n",
    "      #Kernel\n",
    "      K = np.exp( - C/epsilon )\n",
    "      f, g = a, b\n",
    "      print( \"Doing for \", N[i] )\n",
    "      print( \" |- Iterating\" )\n",
    "      start = time.time()\n",
    "      Optimizer = computational_OT.l_bfgs_b(    K,\n",
    "                                                a,\n",
    "                                                b,\n",
    "                                                f,\n",
    "                                                g,\n",
    "                                                epsilon )\n",
    "      out = Optimizer._update()\n",
    "      results_BFGS.append( out )\n",
    "      end = time.time()\n",
    "      times_BFGS.append( end - start )\n",
    "      print( \" |- Computing P\" )\n",
    "      print( \"\" )\n",
    "      u_opt = np.exp( out['potential_f']/epsilon )\n",
    "      K = np.exp( - C/epsilon )\n",
    "      v_opt =  np.exp( out['potential_g']/epsilon )\n",
    "      P_opt = GetP( u_opt, K, v_opt )\n",
    "      BFGSP.append( P_opt )\n",
    "# end for"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Error plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "j92BoYvVKnHf",
    "outputId": "b777c887-caa5-442c-cad1-1941c9ea78b6"
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.title( \"$||P1 -a||_1 + ||P^T 1 -b||_1$\" )\n",
    "for result in results_BFGS:\n",
    "  error = np.asarray( result['error_a'] ) + np.asarray( result['error_b'] ) \n",
    "  plt.plot( error, linewidth = 2 )\n",
    "plt.yscale( 'log' )\n",
    "plt.legend( [ \"N = \"+str(i) for i in N ], loc = \"upper right\" )\n",
    "plt.savefig( \"../Images/Performance_comparison_exisitng_algorithms_images/Convergence_LBGFS.pdf\", format = 'pdf'  )\n",
    "plt.show()\n",
    "print( \"\\n Error plots can increase! The error is not the objective function!\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Objective function plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "zo2Jvy07KnYg",
    "outputId": "cdda91b4-8ce7-4f99-b56b-03470f11b35f"
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.subplot( 2, 1, 1 ),\n",
    "plt.title( \"Objective Function\" )\n",
    "for result in results_BFGS:\n",
    "  plt.plot( np.asarray(result['objective_values']), linewidth = 2 )\n",
    "plt.legend( [ \"N = \"+str(i) for i in N ], loc = \"upper right\" )\n",
    "plt.savefig( \"../Images/Performance_comparison_exisitng_algorithms_images/Objective_LBGFS.pdf\", format = 'pdf'  )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For varying epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFGS\n",
    "print(\"BFGS...\")\n",
    "BFGSP = []\n",
    "results_BFGS = []\n",
    "epsilons = [  0.1, 0.08, 0.05, 0.01 ]\n",
    "N = ( 400, 400 )\n",
    "#Cost matrix\n",
    "C = distmat( x[1], y[1] )\n",
    "# a and b\n",
    "a = normalize( np.ones( N[1] ) )\n",
    "a = a.reshape( a.shape[0], - 1 )\n",
    "b = normalize( np.ones( N[1] ) )\n",
    "b = b.reshape( b.shape[0], - 1 )\n",
    "print( \"Doing for \",(N[1], N[1]) )\n",
    "for eps in epsilons:\n",
    "      #Kernel\n",
    "      K = np.exp( - C/eps )\n",
    "      f, g = a, b\n",
    "      print(\"\\n For epsilon = \" +str(eps) )\n",
    "      print( \" |- Iterating\" )\n",
    "      start = time.time()\n",
    "      Optimizer = computational_OT.l_bfgs_b(    K,\n",
    "                                                a,\n",
    "                                                b,\n",
    "                                                f,\n",
    "                                                g,\n",
    "                                                eps )\n",
    "      out = Optimizer._update()\n",
    "      results_BFGS.append( out )\n",
    "      end = time.time()\n",
    "      print( \" |- Computing P\" )\n",
    "      print( \"\" )\n",
    "      u_opt = np.exp( out['potential_f']/epsilon )\n",
    "      K = np.exp( - C/epsilon )\n",
    "      v_opt =  np.exp( out['potential_g']/epsilon )\n",
    "      P_opt = GetP( u_opt, K, v_opt )\n",
    "      BFGSP.append( P_opt )\n",
    "# end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Error plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.title( \"$$\" )\n",
    "plt.title( \"$||P1 -a||_1+||P^T 1 -b||_1$\" )\n",
    "for i in range( len( results_BFGS ) ):\n",
    "  error = np.asarray(results_BFGS[i]['error_a']) + np.asarray(results_BFGS[i]['error_b'] )\n",
    "  plt.plot( error, label = 'LBGFS for $\\epsilon = $'+ str(epsilons[i]), linewidth = 2 )\n",
    "# end for\n",
    "plt.legend( loc = 'upper right' )\n",
    "plt.yscale( 'log' )\n",
    "plt.savefig( \"../Images/Performance_comparison_exisitng_algorithms_images/LBGFSconvergencevaryepsilon.pdf\", format = 'pdf'  )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OptimalTransportSquarevsAnnulus.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv_computational-OT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
